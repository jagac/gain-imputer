Metadata-Version: 2.1
Name: gain-imputer
Version: 0.0.102
Summary: Missing tabular data imputation using GANs
Home-page: https://github.com/Jagac/gain-imputer
Author: Jagac
Author-email: jagac41@gmail.com
License: MIT
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3.10
Classifier: Operating System :: OS Independent
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: tqdm>=4.66.1
Requires-Dist: numpy>=1.26.2
Requires-Dist: torch>=2.1.2


# GAIN Imputer

[![PyPI version](https://badge.fury.io/py/gain-imputer.svg)](https://badge.fury.io/py/gain-imputer)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## Overview

GAIN (Generative Adversarial Imputation Networks) is a Python library for imputing missing values in a dataset using the GAIN model. It provides a convenient implementation to handle missing data, allowing users to train the imputer and apply it to new data.

This is an adoption of the work done: https://github.com/jsyoon0823/GAIN implemented in torch and provides a familiar workflow to sklearn.


## Installation

```bash
$ pip install gain-imputer
```
## Usage
Gain requires indices of categorical columns to be provided as a list in order to round them properly.

```python
from gain_imputer import GainImputer
cat_columns = [0,1,2]
gain_imputer = GainImputer(
    dim=data_with_nans.shape[1], h_dim=128, cat_columns=cat_columns
)
imputed_data = gain_imputer.fit_transform(data_with_nans)
```

## Full example

```python
from gain_imputer import GainImputer
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score

# Sample data
cancer = load_breast_cancer()
X, y = cancer.data, cancer.target

# Introducing missing data
nan_percentage = 0.5
num_nans = int(np.prod(X.shape) * nan_percentage)
nan_indices = np.random.choice(
    np.arange(np.prod(X.shape)), size=num_nans, replace=False
)
X_flat = X.flatten()
X_flat[nan_indices] = np.nan

X_with_nans = X_flat.reshape(X.shape)
X_train, X_test, y_train, y_test = train_test_split(
    X_with_nans, y, test_size=0.2, random_state=42
)

# GAIN Imputer setup
cat_columns = [0, 1]
gain_imputer = GainImputer(
    dim=X_with_nans.shape[1], h_dim=128, cat_columns=cat_columns, batch_size=1024
)

# Simple fit_transform
imputed_gain_train = gain_imputer.fit_transform(X_train)
imputed_gain_test = gain_imputer.fit_transform(X_test)

# Model training
regressor = RandomForestClassifier(n_jobs=-1)
regressor.fit(imputed_gain_train, y_train)
pred = regressor.predict(imputed_gain_test)

# Evaluate performance
print("Accuracy:", accuracy_score(pred, y_test))
print("F1 Score:", f1_score(pred, y_test))
```

## Reference 

dim: The total number of features or variables in your dataset.

h_dim: The dimensionality of the hidden layer in the GAIN model.

cat_columns: A list of indices representing the categorical columns in your dataset.

batch_size: The size of mini-batches used during training.

hint_rate: The probability of providing hints during training.

alpha: A hyperparameter that balances the generator loss and mean squared error loss during training.

iterations: The number of training iterations or epochs.

